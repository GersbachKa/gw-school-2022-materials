{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pat & Steve's Statistics Smorgasbord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do this if you have not installed astroML yet\n",
    "# !mamba install -c astropy astroML\n",
    "## or this\n",
    "# !pip install astroML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import scipy.stats\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import uniform\n",
    "from astroML import stats as astroMLstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XlYHqLsDXXV"
   },
   "source": [
    "## (1) Descriptive statistics & data moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gstDuWTPbjha"
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "#------------------------------------------------------------\n",
    "# Let's generate some data: a mix of several Cauchy distributions\n",
    "random_state = np.random.RandomState(seed=0)\n",
    "N = 10000\n",
    "mu_gamma_f = [(5, 1.0, 0.1),\n",
    "              (7, 0.5, 0.5),\n",
    "              (9, 0.1, 0.1),\n",
    "              (12, 0.5, 0.2),\n",
    "              (14, 1.0, 0.1)]\n",
    "hx = lambda x: sum([f * scipy.stats.cauchy(mu, gamma).pdf(x)\n",
    "                    for (mu, gamma, f) in mu_gamma_f])\n",
    "data = np.concatenate([scipy.stats.cauchy(mu, gamma).rvs(int(f * N), \n",
    "                                                         random_state=random_state)\n",
    "                       for (mu, gamma, f) in mu_gamma_f])\n",
    "random_state.shuffle(data)\n",
    "data = data[data > -10]\n",
    "data = data[data < 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-be5ypSbmmh"
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# make a histogram to get an idea of what the distribution looks like\n",
    "plt.hist(data, bins=50, density=True, alpha=0.5);\n",
    "plt.xlabel('$x$');\n",
    "plt.ylabel('$f(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DpBlH76bqSa"
   },
   "source": [
    "We all know that the **mean** of a sample is \n",
    "\n",
    "$$\\bar{x} = \\frac{1}{N}\\sum_{i=1}^N x_i$$ \n",
    "\n",
    "This is actually known as the **sample arithmetic mean**, and derives from *Monte Carlo integration* to get the first moment of the distribution, i.e. \n",
    "\n",
    "$$\\mu = E(x) = \\langle x \\rangle = \\int_{-\\infty}^{\\infty} x\\, h(x)\\,dx \\approx \\frac{1}{N}\\sum_{i=1}^N x_i $$\n",
    "\n",
    "where $\\{x_i\\}$ are random samples from the properly normalized $h(x)$, and $E(\\cdot)$ means the **expectation value**. In general we can use random sampling and Monte Carlo integration to deduce integrals over distributions such that \n",
    "\n",
    "$$\\int_{-\\infty}^{\\infty} g(x) h(x)\\,dx \\approx \\frac{1}{N}\\sum_{i=1}^N g(x_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAdPpqZFbwcv"
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "mean = np.mean(data)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VU1mj6F_b0o_"
   },
   "source": [
    "While it's most common to compute the mean, it may surprise you to learn that some distributions do not have formally calculable means (integration gives infinity). In these and other cases, the **median** is a more *robust* estimator of the (true) mean location of the distribution.  That's because it is less affected by **outliers**.\n",
    "\n",
    "To understand the previous statement, think about multiplying all numbers above the 50th percentile (i.e. the median) by 100, or even just replacing them with larger numbers. The mean would be strongly affected by these corrupted points, but **cumulative statistics based on the ordering of samples would remain unaffected by the outlier corruption**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "op9qdNuQb1iL"
   },
   "outputs": [],
   "source": [
    "# Execute this cell.  Think about and discuss what it is doing.\n",
    "median = np.median(data)\n",
    "\n",
    "mask = data > 15\n",
    "data2 = data.copy()\n",
    "data2[mask] = 100\n",
    "\n",
    "newmedian = np.median(data2)\n",
    "newmean = np.mean(data2)\n",
    "\n",
    "print(median, newmedian)\n",
    "print(mean, newmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KqdtNLtb5dJ"
   },
   "source": [
    "<font color='red'>Repeat the above masking investigation, but this time multiply all samples above $15$ by a factor of 10. Do you get a similar effect?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_zuhtBwg0mH"
   },
   "outputs": [],
   "source": [
    "# Execute this cell.  Think about and discuss what it is doing.\n",
    "median = np.median(data)\n",
    "\n",
    "mask = data > 15\n",
    "data2 = data.copy()\n",
    "data2[mask] *= 15\n",
    "\n",
    "newmedian = np.median(data2)\n",
    "newmean = np.mean(data2)\n",
    "\n",
    "print(median, newmedian)\n",
    "print(mean, newmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-V1Kzs3Ib-8T"
   },
   "source": [
    "Other descriptive statistics are related to higher order moments of the distribution. Beyond the \"average\" *location* value, we'd like to know something about **deviations** from the average (which is related to the *shape* of the distribution).  The simplest thing to compute is $$d_i = x_i - \\mu.$$  However, the average deviation is zero by definition of the mean.  The next simplest thing to do is to compute the **mean absolute deviation (MAD)**:\n",
    "\n",
    "$$\\frac{1}{N}\\sum|x_i-\\mu|,$$\n",
    "\n",
    "but the absolute values can hide the true scatter of the distribution [in some cases (see footnote)](http://www.mathsisfun.com/data/standard-deviation.html).  So the next simplest thing to do is to square the differences $$\\sigma^2 = \\frac{1}{N}\\sum(x_i-\\mu)^2,$$ which we call the **variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwQwHhB-cBFg"
   },
   "source": [
    "The *variance* $V$ is just expectation value of $(x-\\mu)^2$ (and related to the 2nd moment)\n",
    "\n",
    "$$\\sigma^2 = V = E((x-\\mu)^2)\\int_{-\\infty}^{\\infty}  (x-\\mu)^2 h(x) dx,$$\n",
    "\n",
    "where $\\sigma$ is the **standard deviation**. Again, the integral gets replaced by a sum for discrete distributions. While most familiar for Gaussian distributions, you can compute the variance even if your distribution is not Gaussian.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kd4-ghpccEAN"
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "var = np.var(data)\n",
    "std = np.std(data)\n",
    "print(var, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5Z-nANocI5Y"
   },
   "source": [
    "**$P\\%$ quantiles (or the $p^\\mathrm{th}$ percentile, $q_p$)** are computed as\n",
    "$$\\frac{p}{100} = H(q_p) = \\int_{-\\infty}^{q_p}h(x) dx$$\n",
    "\n",
    "The full integral from $-\\infty$ to $\\infty$ is 1 (100%).  So, here you are looking for the value of x that accounts for $p$ percent of the distribution.\n",
    "\n",
    "For example, the 25th, 50th, and 75th percentiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gb0ZsBZb6aI"
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "q25, q50, q75 = np.percentile(data, [25, 50, 75])\n",
    "print(q25, q50, q75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-r67Vk5DcOHF"
   },
   "source": [
    "The **interquartile range** is the difference between the 25th and 75th percentiles, $q_{75} - q_{25}$.\n",
    "\n",
    "Just as with the median, the interquartile range is a more *robust* estimator of the scale of a distribution than the standard deviation.  So, one can create a standard-deviation-esque measurement (at least for a Gaussian) from the interquartile range as\n",
    "\n",
    "$$\\sigma_G = 0.7413\\times(q_{75} - q_{25})$$  \n",
    "\n",
    "The normalization makes it *unbiased* for a perfect Gaussian (more on that later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QB7-ODlAcUoj"
   },
   "source": [
    "Other useful ***shape*** measures include the \"higher order\" moments (the **skewness** and **kurtosis**):\n",
    "\n",
    "$$\\mathbf{Skewness}\\quad\\quad \\Sigma = \\int_{-\\infty}^{\\infty}  \\left(\\frac{x-\\mu}{\\sigma}\\right)^3 h(x) dx,$$\n",
    " \n",
    "$$\\mathbf{Kurtosis}\\quad\\quad K = \\int_{-\\infty}^{\\infty}  \\left(\\frac{x-\\mu}{\\sigma}\\right)^4 h(x) dx  - 3.$$\n",
    "\n",
    "The skewness measures the distribution's *asymmetry*. Distribution's with long tails to larger $x$ values have positive $\\Sigma$. \n",
    "\n",
    "The kurtosis measures how peaked or flat-topped a distribution is, with strongly peaked ones being positive and flat-topped ones being negative. $K$ is calibrated to a Gaussian distribution (hence the subtraction of $3$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APWeMooGcX7T"
   },
   "source": [
    "![https://www.astroml.org/_images/fig_kurtosis_skew_1.png](https://www.astroml.org/_images/fig_kurtosis_skew_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1uN-cC8ycTQw"
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "skew = scipy.stats.skew(data)\n",
    "kurt = scipy.stats.kurtosis(data)\n",
    "print(skew, kurt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_pQRMWXcc2D"
   },
   "outputs": [],
   "source": [
    "# Excute this cell\n",
    "# Summary descriptive statistics for our distribution\n",
    "print(\"Location: \", mean, median)\n",
    "print(\"Scale: \", var, std, astroMLstats.sigmaG(data))\n",
    "print(\"Shape: \", skew, kurt)\n",
    "print(\"Some percentiles: \", q25, q50, q75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGpkYIDcdSe4"
   },
   "source": [
    "## (2) Univariate distributions <a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "If we are attempting to characterize our data in a way that is **parameterized**, then we need a functional form for a **distribution**.  There are many naturally occurring distributions.  The book goes through quite a few of them.  Here we'll just talk about a few basic ones to get us started.\n",
    "\n",
    "### Uniform Distribution\n",
    "\n",
    "The uniform distribution is perhaps more commonly called a \"top-hat\" or a \"box\" distribution.  It is specified by a mean, $\\mu$, and a width, $W$, where\n",
    "\n",
    "$$p(x|\\mu,W) = \\frac{1}{W}$$\n",
    "\n",
    "over the range $|x-\\mu|\\le \\frac{W}{2}$ and $0$ otherwise.  That says that \"given $\\mu$ AND $W$, the probability of $x$ is $\\frac{1}{W}$\" (as long as we are within a certain range).\n",
    "\n",
    "Since we are used to thinking of a Gaussian as the *only* type of distribution the concept of $\\sigma$ (aside from the width) may seem strange.  But $\\sigma$ as mathematically defined above applies here and\n",
    "$$\\sigma = \\frac{W}{\\sqrt{12}}.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06EtEOZod1Sd"
   },
   "source": [
    "\n",
    "We can implement [uniform](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.uniform.html#scipy.stats.uniform) in `scipy` as follows.  We'll use the methods listed at the bottom of the link to complete the cell: `dist.rvs(size=N)` which produces `N` random draws from the distribution and `dist.pdf(x)` which returns the value of the pdf at a given $x$. Lots of distributions can be accessed and used in a similar way.  \n",
    "\n",
    "Create a uniform distribution with parameters `loc=0`,  `scale=2`, and `N=10`.\n",
    "\n",
    "<font color='red'>Complete and execute the following cell</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDt5tfdVd7OC"
   },
   "outputs": [],
   "source": [
    "\n",
    "N = ____ # Complete\n",
    "distU = scipy.stats.uniform(____,____) # Complete\n",
    "draws = distU.rvs(____) # ten random draws\n",
    "print(draws)\n",
    "\n",
    "p = distU.pdf(____) # pdf evaluated at x=1\n",
    "\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTtuHZ65d-8x"
   },
   "source": [
    "\n",
    "### Gaussian Distribution\n",
    "\n",
    "As many of you know, the Gaussian distribution pdf is given by\n",
    "\n",
    "$$p(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x-\\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "It is also called the **normal distribution** and can be noted by $\\mathscr{N}(\\mu,\\sigma)$.\n",
    "\n",
    "\n",
    "We love using Gaussians in physics and astronomy because they can approximate many distributions and are also super easy to work with. **The convolution of two Gaussians results in a Gaussian.**  So $\\mathscr{N}(\\mu_1,\\sigma_1)$ convolved with $\\mathscr{N}(\\mu_2,\\sigma_2)$ is $\\mathscr{N}(\\mu_1+\\mu_2,\\sqrt{\\sigma_1^2+\\sigma_2^2})$.\n",
    "\n",
    "Create a [normal distribution](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html?highlight=stats%20norm#scipy.stats.norm) with `loc=100` and `scale=15`. Produce 10 random draws and determine the probability at `x=145`.\n",
    "\n",
    "<font color='red'>Complete and execute the following cell</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUjjN01feK8D"
   },
   "outputs": [],
   "source": [
    "\n",
    "distG = scipy.stats.norm(____,____) # Normal distribution with mean = 100, stdev = 15\n",
    "draws = ____.____(____) # 10 random draws\n",
    "p = ____.____(____) # pdf evaluated at x=0\n",
    "\n",
    "print(draws)\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rCepaYmeNhN"
   },
   "source": [
    "\n",
    "Make a plot of this Gaussian distribution. Plot the pdf from 0 to 200 with a 1000 gridpoints.  \n",
    "\n",
    "<font color='red'>Complete and execute the following cell</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02w9fuT6eSyF"
   },
   "outputs": [],
   "source": [
    "\n",
    "## Let's play with Gaussians! Or Normal distributions, N(mu,sigma)\n",
    "\n",
    "xgrid = np.linspace(____,____,____) # generate distribution for a uniform grid of x values\n",
    "____ = distG.pdf(____)  # this is a function of xgrid\n",
    "\n",
    "# actual plotting\n",
    "fig, ax = plt.subplots(figsize=(5, 3.75))\n",
    "\n",
    "# Python3 f strings are awesome!\n",
    "plt.plot(xgrid, gaussPDF, ls='-', c='black', \n",
    "         label=f'$\\mu={mu},\\ \\sigma={sigma}$')\n",
    "plt.xlim(0, 200)\n",
    "plt.ylim(0, 0.03)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r'$p(x|\\mu,\\sigma)$')\n",
    "plt.title('Gaussian Distribution')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfNRO-SveWjN"
   },
   "source": [
    "\n",
    "The cumulative distribution function, cdf is the integral of pdf from $x'=-\\infty$ to $x'=x$:\n",
    "\n",
    "$$\\mathrm{cdf}(x|\\mu,\\sigma) = \\int_{-\\infty}^{x'} p(x'|\\mu,\\sigma) dx',$$\n",
    "\n",
    "where $\\mathrm{cdf}(\\infty) = 1$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTTXSzT-eadq"
   },
   "outputs": [],
   "source": [
    "\n",
    "# The same as above but now with the cdf method\n",
    "gaussCDF = distG.cdf(xgrid)\n",
    "fig, ax = plt.subplots(figsize=(5, 3.75))\n",
    "plt.plot(xgrid, gaussCDF, ls='-', c='black', \n",
    "         label=r'$\\mu=%i,\\ \\sigma=%i$' % (mu, sigma))\n",
    "plt.xlim(0, 200)\n",
    "plt.ylim(-0.01, 1.01)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r'$CDF(x|\\mu,\\sigma)$')\n",
    "plt.title('Gaussian Distribution')\n",
    "plt.legend(loc=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPIE-XsNecqi"
   },
   "source": [
    "\n",
    "#### Gaussian confidence levels\n",
    "\n",
    "The probability of a measurement drawn from a Gaussian distribution that is between $\\mu-a$ and $\\mu+b$ is\n",
    "\n",
    "$$\\int_{\\mu-a}^{\\mu+b} p(x|\\mu,\\sigma) dx.$$\n",
    "\n",
    "- For $a=b=1\\sigma$, we get the familar result of 68.3%.  \n",
    "- For $a=b=2\\sigma$ it is 95.4%.  \n",
    "- For $a=b=3\\sigma$ it is $99.7\\%$. \n",
    "\n",
    "So we refer to the range $\\mu \\pm 1\\sigma$, $\\mu \\pm 2\\sigma$, and $\\mu \\pm 3\\sigma$ as the 68%, 95%, and $99.7%$ **confidence limits**, respectively. Note that if your distribution is not Gaussian, then these confidence intervals will be different!\n",
    "\n",
    "***We often still refer to uncertainty regions of distributions as $1\\sigma$ or $2\\sigma$ regions, which for non-Gaussian distributions usually means (for $1\\sigma$) the region enclosing the $16\\%$ and $84\\%$ quantiles.***\n",
    "\n",
    "What is the probability enclosed between $-2\\sigma$ and $+4\\sigma$? (*Verify first that you get the correct answer for the bullet points above!*)\n",
    "\n",
    "<font color='red'>Complete and execute the following cell</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKE-vYlBehnf"
   },
   "outputs": [],
   "source": [
    "\n",
    "N=10000\n",
    "mu=0\n",
    "sigma=1\n",
    "distN = ___.___.___(mu, sigma) # Complete\n",
    "xgrid = np.linspace(___,___,N) # Complete\n",
    "dx = (xgrid.max()-xgrid.min())/N\n",
    "prob = distN.pdf(xgrid)*dx\n",
    "\n",
    "print(prob.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5VnEm2ZelfB"
   },
   "source": [
    "\n",
    "We could do this in various ways. The way you just tried was the most obvious-- brute-force numerical integration with the trapezoidal method. \n",
    "\n",
    "But the clever way is to use the cdf, by computing the cdf of the upper integration bound and subtracting the cdf of the lower integration bound.\n",
    "\n",
    "<font color='red'>Complete and execute the following cell</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRVgS7Sgesf4"
   },
   "outputs": [],
   "source": [
    "\n",
    "upper = distN.cdf(___)\n",
    "lower = distN.cdf(___)\n",
    "p = upper-lower\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CERd1D1Se3N4"
   },
   "source": [
    "\n",
    "\n",
    "### $\\chi^2$ Distribution\n",
    "\n",
    "We'll run into the $\\chi^2$ distribution when we talk about Maximum Likelihood in the next chapter.\n",
    "\n",
    "If we have a Gaussian distribution with values ${x_i}$ and we scale and normalize them according to\n",
    "$$z_i = \\frac{x_i-\\mu}{\\sigma},$$\n",
    "then the sum of squares, $Q$ \n",
    "$$Q = \\sum_{i=1}^N z_i^2,$$\n",
    "will follow the $\\chi^2$ distribution.  The *number of degrees of freedom*, $k$ is given by the number of data points, $N$ (minus any constraints).  The pdf of $Q$ given $k$ defines $\\chi^2$ and is given by\n",
    "$$p(Q|k)\\equiv \\chi^2(Q|k) = \\frac{1}{2^{k/2}\\Gamma(k/2)}Q^{k/2-1}\\exp(-Q/2),$$\n",
    "where $Q>0$ and the $\\Gamma$ function would just be the usual factorial function if we were dealing with integers, but here we have half integers.\n",
    "\n",
    "This is ugly, but it is really just a formula like anything else.  Note that the shape of the distribution *only* depends on the sample size $N=k$ and not on $\\mu$ or $\\sigma$.  \n",
    "\n",
    "### Chi-squared per degree of freedom\n",
    "\n",
    "In practice we frequently divide $\\chi^2$ by the number of degrees of freedom, and work with:\n",
    "\n",
    "$$\\chi^2_\\mathrm{dof} = \\frac{1}{N-1} \\sum_{i=1}^N \\left(\\frac{x_i-\\overline{x}}{\\sigma}\\right)^2$$\n",
    "\n",
    "which (for large $k$) is distributed as\n",
    "\n",
    "$$ p(\\chi^2_\\mathrm{dof}) \\sim \\mathscr{N}\\left(1, \\sqrt{\\frac{2}{N-1}}\\right) $$\n",
    "\n",
    "(where $k = N-1$, and $N$ is the number of samples). Therefore, we expect $\\chi^2_\\mathrm{dof}$ to be 1, to within a few $\\sqrt{\\frac{2}{N-1}}$.\n",
    "\n",
    "### Poisson distribution\n",
    "\n",
    "This is a distribution for a discrete variable, telling you the probability of $k$ events occuring within a certain time when the mean is $\\mu$. \n",
    "\n",
    "An early and famous example of the use of this distribution was to **model the expected number of Prussian cavalrymen that would be kicked to death by their own horse**. Statistics has many applications...\n",
    "\n",
    "$$ p(k|\\mu) = \\frac{\\mu^k \\exp(-\\mu)}{k!} $$\n",
    "\n",
    "where the mean $\\mu$ completely characterizes the distribution. The mode is $(\\mu-1)$, the standard deviation is $\\sqrt{\\mu}$, the skewness is $1/\\sqrt{\\mu}$, and the kurtosis is $1/\\mu$.\n",
    "\n",
    "As $\\mu$ increases the Poisson distribution becomes more and more similar to a Gaussian with $\\mathcal{N}(\\mu,\\sqrt{\\mu})$. The Poisson distribution is sometimes called the ***law of small numbers*** or ***law of rare events***.\n",
    "\n",
    "<font color='red'>Complete and execute the following cell for $\\mu=3$.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-GciuWXfcCp"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Practice the Poisson distribution\n",
    "\n",
    "dist = scipy.stats.___(___)\n",
    "\n",
    "k = dist.rvs(___) # make 20 draws\n",
    "pmf = dist.___(6) # evaluate probability mass function at 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAR4boS9feG2"
   },
   "source": [
    "\n",
    "### Student's $t$ Distribution\n",
    "\n",
    "Another distribution that we'll see later is the Student's $t$ Distribution.\n",
    "\n",
    "If you have a sample of $N$ measurements, $\\{x_i\\}$, drawn from a Gaussian distribution, $\\mathscr{N}(\\mu,\\sigma)$, and you apply the transform\n",
    "\n",
    "$$t = \\frac{\\overline{x}-\\mu}{s/\\sqrt{N}},$$\n",
    "\n",
    "then $t$ will be distributed according to Student's $t$ with the following pdf (for $k$ degrees of freedom):\n",
    "\n",
    "$$p(x|k) = \\frac{\\Gamma(\\frac{k+1}{2})}{\\sqrt{\\pi k} \\Gamma(\\frac{k}{2})} \\left(1+\\frac{x^2}{k}\\right)^{-\\frac{k+1}{2}}$$\n",
    "\n",
    "As with a Gaussian, Student's $t$ is bell shaped, but has \"heavier\" tails.\n",
    "\n",
    "Note the similarity between $t$ and $z$ for a Gaussian (as defined in the $\\chi^2$ section above), which reflects the difference between data-derived estimates of the mean and standard deviation and their true values.\n",
    "\n",
    "In fact, although often approximated as a Gaussian distribution, the mean of a sample actually follows a Student's $t$ distribution. This matters when sample sizes are small, but mostly irrelevant for \"Big Data\" examples.\n",
    "\n",
    "### What's the point of all these distributions?\n",
    "\n",
    "* There are many other distributions that we haven't covered here.\n",
    "* The point is that we want make measurements. \n",
    "* To understand the significance of our measurement, we want to know how likely it is that we would get that measurement in our experiment by random chance. \n",
    "* To determine that we need to know the shape of the distribution. Let's say that we find that $x=6$. If our data is $\\chi^2$ distributed with 2 degrees of freedom, then we would integrate the $k=2$ curve above from 6 to $\\infty$ to determine how likely it is that we would have gotten 6 or larger by chance.  If our distribution was instead $t$ distributed, we would get a *very* different answer.  \n",
    "\n",
    "**FUN FACT:** \"Student\" was the pen name of W. S. Gosset, who worked for the Guinness brewery in Dublin, Ireland. He was interested in the statistical analysis of small samples, e.g., the chemical properties of barley when the sample size might be as small as $3$.\n",
    "\n",
    "![https://thatsmaths.files.wordpress.com/2014/04/gosset-plaque.jpg](https://thatsmaths.files.wordpress.com/2014/04/gosset-plaque.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QR2dtsfQJJy"
   },
   "source": [
    "## (3) Central Limit Theorem <a class=\"anchor\" id=\"one\"></a>\n",
    "\n",
    "One of the reasons that a Gaussian (or Normal) Distribution is so common is because of the **Central Limit Theorem**:\n",
    "\n",
    "> For an arbitrary distribution, $h(x)$, with a well-defined mean, $\\mu$, and standard deviation, $\\sigma$ (i.e. tails should fall off faster than $1/x^2$) the mean of $N$ values \\{$x_i$\\} drawn from the distribution will follow a Gaussian Distribution with $\\mathcal{N}(\\mu,\\sigma/\\sqrt{N})$. (A Cauchy distribution is one example where this fails.)\n",
    "\n",
    "This theorem is the foundation for performing repeat measurements in order to improve the accuracy of one's experiment. This is truly amazing! No matter what distribution you start off with (provided it has a well defined mean and standard deviation) or the measurement process itself, repeated batches of $N$ draws will follow a Gaussian centered around the mean.  \n",
    "\n",
    "The **Weak Law of Large Numbers** (aka **Bernoulli's Theorem**) further says that the sample mean will converge to the distribution mean as $N$ increases.\n",
    "\n",
    "Let's wrap our heads around what this means with some examples. \n",
    "\n",
    "We'll first consider $h(x) = \\mathcal{N}(\\mu=0.5,\\sigma=1/\\sqrt{12})$. According to the Central Limit Theorem, taking the mean of many batches of $N$ random samples should result in a normal distribution with $\\mathcal{N}(\\mu=0.5,\\sigma=1/\\sqrt{12}/\\sqrt{N})$.\n",
    "\n",
    "<font color='red'>Complete and execute the following cell</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFJRB3fYQN0I"
   },
   "outputs": [],
   "source": [
    "\n",
    "N = 2 # Number of draws\n",
    "mu = 0.5 # Location\n",
    "sigma_h = 1.0 / np.sqrt(12) # scale of h(x)\n",
    "\n",
    "xgrid = ___.___(___,___,1000) # Array to sample the space \n",
    "distG = ___.___(___, ___) # Complete\n",
    "plt.plot(___, ___.___(___)) # Complete\n",
    "\n",
    "x = np.random.normal(mu, sigma_h,2) # Two random draws\n",
    "plt.plot(x, 0*x, '|', markersize=50)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKrmh5YmQSxQ"
   },
   "source": [
    "\n",
    "Now let's average those two draws and plot the result (in the same panel). Do it as a histogram for 100,000 batches of 2 samples each. Use a stepfilled histogram that is normalized with 50% transparency and 100 bins.\n",
    "\n",
    "<font color='red'>Complete and execute the following cell</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hu3d51vXQqi9"
   },
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "N = 2 # Number of draws\n",
    "mu = 0.5 # Location\n",
    "sigma_h = 1.0 / np.sqrt(12) # scale of h(x)\n",
    "sigma_cl = sigma_h / np.sqrt(N) # scale of mean error distribution\n",
    "\n",
    "xgrid = np.linspace(___,___,___) # Array to sample the space \n",
    "\n",
    "# plot the distribution of means according to central limit theorem\n",
    "distG = stats.norm(mu, sigma_cl) # Complete\n",
    "plt.plot(xgrid, distG.pdf(xgrid)) # Complete\n",
    "\n",
    "# Add a histogram that is the mean of 100,000 batches of N draws\n",
    "yy = []\n",
    "for i in np.arange(100000):\n",
    "    xx = np.random.normal(___, ___, ___) # N random draws\n",
    "    yy.append(___) # Append average of those random draws to the end of the array\n",
    "\n",
    "_ = plt.hist(yy, bins=100, histtype='stepfilled', alpha=0.5, density=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zMPmzSKQvhg"
   },
   "source": [
    "\n",
    "Well that's great and all, but didn't I say this worked for arbitrary generating distributions $h(x)$ so long as their mean and standard deviations were well defined? Let's check this out for a uniform distribution with $\\mu=0.5$ and width$ =1$.\n",
    "\n",
    "<font color='red'>Complete the following cell</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXU0QeVjQ5e3"
   },
   "outputs": [],
   "source": [
    "\n",
    "N = 2 # Number of draws\n",
    "mu = 0.5 # Location\n",
    "sigma_h = 1.0 / np.sqrt(12) # scale of h(x)\n",
    "sigma_cl = sigma_h / np.sqrt(N) # scale of mean error distribution\n",
    "\n",
    "# Complete the rest of this cell with a repetition of the the previous exercise,\n",
    "# except drawing N-sample batches from a [0,1] uniform distribution, and plotting\n",
    "# the distribution of the means of those batches. Show that the distribution\n",
    "# matches that implied by the central limit theorem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQvDUk0rQ9oW"
   },
   "source": [
    "\n",
    "Now instead of averaging 2 draws, average 3.  Then do it for 10.  Then for 100.  Each time for 100,000 samples.\n",
    "\n",
    "\n",
    "<font color='red'>Copy your code from above and edit accordingly (or just edit your code from above)</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dj_65wLNRFIz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OrXBqplRDCD"
   },
   "source": [
    "\n",
    "\n",
    "For 100 you will note that your draws are clearly sampling the full range, but the means of those draws are in a *much* more restrictred range. Moreover they are very closely following a Normal Distribution. \n",
    "\n",
    "This is truly mind blowing, and a wonderful example of the power and generalizability of statistics in tackling the measured properties of arbitrary distributions. Even if you've never heard of or understood the Central Limit Theorem, you have been implicitly using it your entire career so far. \n",
    "\n",
    "If you are confused, then watch this video from the Khan Academy:\n",
    "[https://www.khanacademy.org/math/statistics-probability/sampling-distributions-library/sample-means/v/central-limit-theorem](https://www.khanacademy.org/math/statistics-probability/sampling-distributions-library/sample-means/v/central-limit-theorem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aj2ufUJuRbfu"
   },
   "source": [
    "## (4) Sampling from arbitrary distributions <a class=\"anchor\" id=\"four\"></a>\n",
    "\n",
    "Numerical simulations of the measurment process are often the only practical way to assess the complicated influences of selection effects, biases, and other processing stages of data taking. Such approaches are often called **Monte Carlo simulations**, producing **Monte Carlo or mock samples**. It is often highly beneficial to resample from arbitrary distributions in order to simulate further measurements.\n",
    "\n",
    "Imagine you've got an arbitrary distribution $h(x)$ in analytic or histogram form (or alternatively just samples from it). How do you make more samples if it's not one of the standard distributions in `numpy` or `scipy`? We'll discuss the 1D case here in two ways. **Rejection sampling** (less preferred) and **Inverse transform sampling** (highly preferred and easy).\n",
    "\n",
    "We'll use our samples drawn from a mix of Cauchy distributions (from the top of the notebook) in these examples.\n",
    "\n",
    "<font color='red'>Make a numpy histogram object out of the data with `density=True` and `bins=50`. This will return a tuple of bin heights and bin edges.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5GVqBqt6SJKP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3p_0QrT9SGm6"
   },
   "source": [
    "\n",
    "### Rejection sampling\n",
    "\n",
    "The process for sampling from an arbitrary distribution with rejection is:\n",
    "\n",
    "1) Decide on a straightforward *proposal distribution* $q(x)$ to propose new samples. It should be wide enough to capture the tails of $h(x)$. We'll use a uniform distribution here.\n",
    "\n",
    "2) Generate a random sample from $q(x)$, $x_i$.\n",
    "\n",
    "3) Now generate a random sample, $u$, from a uniform distribution in the range $[0,\\mathrm{max}(h(x))]$, where the upper bound should be as large or larger than the maximum density of $h(x)$. (This could be worked out analytically or by histograming the data.)\n",
    "\n",
    "4) If $u\\leq h(x_i)$ accept the point, or else reject it and try again from step 2.\n",
    "\n",
    "\n",
    "This is shown pictorally here\n",
    "\n",
    "![image.png](https://thatnerd2.github.io/img/reject.png)\n",
    "\n",
    "Let's try this for a simple Gaussian.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjW_b7KtS570"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Execute this cell\n",
    "\n",
    "# make some sample data\n",
    "sample_data = np.random.normal(1.0,0.2,10000)\n",
    "\n",
    "# make a simple histogram object\n",
    "counts, bins = np.histogram(sample_data, bins=50, density=True)\n",
    "maxh = counts.max() # find the maximum\n",
    "\n",
    "# Make a scipy.stats random variable object from a histogram\n",
    "# This is a great hack!\n",
    "disth = scipy.stats.rv_histogram((counts,bins))\n",
    "\n",
    "N = 100000 # trials\n",
    "q = np.random.uniform(0.0, 2.0, N) # proposed points\n",
    "u = np.random.uniform(0, maxh, N) # uniform draws\n",
    "\n",
    "mask = u<=disth.pdf(q) # assess whether u <= q(x_i) in the smart pythonic way\n",
    "\n",
    "monte_carlo = q[mask] # reject all points that don't pass, using masking\n",
    "\n",
    "plt.hist(monte_carlo, bins=50, density=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1fy0vbgS86Q"
   },
   "source": [
    "\n",
    "...of course once you have the `disth` you can immediately draw from the pdf using `disth.rvs()`. This is worth knowing for your own research. It's a great hack, but it's important to know the theory behind all this.\n",
    "\n",
    "<font color='red'>Use this process to generate at least $1000$ new samples of the Cauchy-mix (i.e. the `data` sample), and make a plot that shows histograms of the original data and the new Monte Carlo samples to compare.</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flsW-sNqTDih"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVRtBCK1TBhd"
   },
   "source": [
    "\n",
    "\n",
    "### Inverse transform sampling\n",
    "\n",
    "Rejection sampling works, but wouldn't it be awesome if we didn't have to discard *any* points during our sampling? This is the power and simplicity of **inverse transform sampling**. The process is exceedingly simple. \n",
    "\n",
    "1) To sample from an arbitrary $h(x)$, you will also need the cdf $H(x)$ and its inverse, the quantile function $H^{-1}(x)$. If $h(x)$ is difficult to integrate, or $H(x)$ is difficult to invert to get the quantile function, then you could use numerical techniques to interpolate or produce lookup tables.\n",
    "\n",
    "2) Generate a random sample $u$ from a uniform random distribution $[0,1]$. \n",
    "\n",
    "3) Using the quantile function $H^{-1}(x)$, find the value of $x$ below which a fraction $u$ of the distribution is contained. \n",
    "\n",
    "4) The $x$ value you get is a random sample from $h(x)$. Easy, right?\n",
    "\n",
    "Let's try with the Gaussian again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01eb0bV2TFAW"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Execute this cell\n",
    "\n",
    "# make some sample data\n",
    "sample_data = np.random.normal(1.0,0.2,10000)\n",
    "\n",
    "# make a simple histogram object\n",
    "counts, bins = np.histogram(sample_data, bins=50, density=True)\n",
    "bin_mids = (bins[1:] + bins[:-1]) / 2 # mid location of bins\n",
    " \n",
    "simple_cdf = np.cumsum(counts) / np.sum(counts) # very simply cumulative sum\n",
    "\n",
    "# set up an interpolation of the inverse cumulative distribution\n",
    "tck = scipy.interpolate.interp1d(simple_cdf, bin_mids)\n",
    "\n",
    "# sample evenly along the cumulative distribution, and interpolate\n",
    "# little hack to make sure no points are generated outside interpolation range.\n",
    "# not ideal\n",
    "u = np.random.uniform(0.001, 0.999, 1000) \n",
    "x_sample = tck(u)\n",
    "\n",
    "plt.hist(x_sample, bins=50, density=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWtkbjCZTH3B"
   },
   "source": [
    "\n",
    "<font color='red'>Use this process to generate exactly $1000$ new samples of the Cauchy-mix (i.e. the `data` sample), and make a plot that shows histograms of the original data and the new Monte Carlo samples to compare.</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbzctcQKTKNP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0h4OQJK9RaPE"
   },
   "source": [
    "\n",
    "\n",
    "## (5) Stochastic Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b9W9AIQ_7Jh"
   },
   "source": [
    "The most intuitive *starting* discussion for stochatic processes I've found over the years is here: https://www.youtube.com/watch?v=TuTmC8aOQJE&t=689s&ab_channel=MITOpenCourseWare, so I'll be following along from there to start, and then will diverge a bit into some more practical pieces.\n",
    "\n",
    "A stochastic process is just a time-ordered set of random variables, $\\{X_t\\}$. It can be either discrete or continuous. For example, a random collection of values...\n",
    "\n",
    "![screenshot](https://drive.google.com/uc?export=view&id=1YyMLvir0PjLjL7Ofyi_dgQXdybdrT-_x)\n",
    "\n",
    "Another way of thinking of this is a \"probability distribution over paths.\" Let's look at three examples:\n",
    "\n",
    "* First: $f(t) = t$ with probability 1. Well, this is just a normal function.\n",
    "\n",
    "* Alternatively, we could have $f(t) = t$ with probability 1/2 and $f(t)=-t$ with probability 1/2. This is a continuous stochastic process where, when we know the value at one time, we know it at all times. But we have given a simple probability assigned to two separate paths.\n",
    "\n",
    "* One final example, would be for each $t$, $f(t)=t$ or $f(t) = -t$, each with probability 1/2. So that at each point we sit on one or the other of the two deterministic functions. \n",
    "\n",
    "Let's evaluate this last function. Note that, because this is a stochastic process, the path we take is inherently **probabilistic**, and so if you re-run that cell multiple times, you will find a different path. Even though this is a continuous-time stochastic process, we can still evaluate it at a set of discrete points. Try running the cell below several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zLwgI3zD0dl"
   },
   "outputs": [],
   "source": [
    "discrete_times = np.linspace(0, 10, num=11) # times from 0 to 100\n",
    "random_path = []\n",
    "for ii, t in enumerate(discrete_times):\n",
    "    # randomly choose from t or -t with equal probability, add it to our stochastic\n",
    "    # process\n",
    "    random_path.append(np.random.choice([-t, t]))\n",
    "plt.plot(discrete_times, random_path, '-o', c='k')\n",
    "plt.xlabel('Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eiB57yhG27S"
   },
   "source": [
    "In things like finance, they are interested in **forecasting**...so if they know the path up to some time, they'd like to be able to predict what happens in the future. So they're looking to understand some underlying probabilistic behaviour they can use to predict the future and make money. In the end, even though we're not interested in predicting something about the future, for PTAs we'd also like to understand if that probabibilistic behaviour **matches what we expect**. \n",
    "\n",
    "So we'll explore how to take a stochastic process (e.g. pulsar timing residuals), and characterize to understand it.\n",
    "\n",
    "### Random walk\n",
    "\n",
    "Let's consider a simple random walk. Define $Y_i = 1$ or $Y_i=-1$ each with probability 1/2. Then define \n",
    "\n",
    "$$X_t = \\sum_{i=0}^{i=t}Y_i.$$\n",
    "\n",
    "While this results in a random, different path each time, the **central limit theorem** actually tells us the probability distribution of our position after a long walk. After a long time $\\tau$ the value of $X_\\tau$ will, on average have a value zero, but a variance of $\\tau$. Let's check this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1MNivrhGck8"
   },
   "outputs": [],
   "source": [
    "def random_walk(Nsteps):\n",
    "    Y_is = np.array([np.random.choice([1, -1]) for ii in range(Nsteps)])\n",
    "    rw = np.cumsum(Y_is)\n",
    "    return rw\n",
    "Nsteps = 1000\n",
    "Nsimulations = 1000\n",
    "for ii in range(Nsimulations):\n",
    "    plt.plot(random_walk(Nsteps), c='k', alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p483hIBdK71c"
   },
   "source": [
    "Exercise! Pull out the last value of each realization of the random walk, and create a histogram. Does it look Gaussian? What is the mean of those values? Its variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03tvLLZvKq0B"
   },
   "outputs": [],
   "source": [
    "# Take the ending value of 1000 realisations of a random walk (using random_walk function)\n",
    "# either in a list comprehension or a loop\n",
    "random_walk_last_values = ________ \n",
    "plt.hist(random_walk_last_values)\n",
    "plt.show()\n",
    "\n",
    "print(np.mean(random_walk_last_values))\n",
    "print(np.var(random_walk_last_values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYVev-hsMu1B"
   },
   "source": [
    "### Autocorrelation function\n",
    "\n",
    "Ok, let's now define the autocorrelation function. This is the expectation value of the product of our stochastic process at one time with the process at a different time. If we subtract off the mean first, it's called the \"auto-covariance function.\"\n",
    "\n",
    "Autocorrelation function: $R(X_{t_1}, X_{t_2}) = E[X_{t_1} X_{t_2}].$\n",
    "\n",
    "\n",
    "Auto-covariance function: $R(X_{t_1}, X_{t_2}) = E[(X_{t_1}-\\mu_{t_1})(X_{t_2}-\\mu_{t_2})].$\n",
    "\n",
    "If the autocorrelation function depends only on the time difference between the two points then it is called \"**wide-sense stationary**\". A nice property of wide-sense stationary processes is that the mean and auto-covariance are time-independent, and so is the variance,\n",
    "\n",
    "$$R(\\tau) = E[X_t X_{t+\\tau}].$$\n",
    "\n",
    "This is what we think of when we think of \"random noise\" usually. If we have 10 seconds of data from some detector, then the variance will be the same as if we took 100 seconds of data from the same.\n",
    "\n",
    "Let's look at this for our random walk, though!\n",
    "\n",
    "Calculate the average of the product $X_{10}X_{15}$ over many realisations of our random walk. What does it come out to? Try to change the values from 10 and 15 to 30 and 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCL6ZsgATJG3"
   },
   "outputs": [],
   "source": [
    "corrs_10_15 = []\n",
    "corrs_30_50 = []\n",
    "for ii in range(Nsimulations):\n",
    "    rw = random_walk(51)\n",
    "    corrs_10_15.append(____)\n",
    "    corrs_30_50.append(____)\n",
    "print(np.mean(corrs_10_15))\n",
    "print(np.mean(corrs_30_50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_mrXhxbTJZj"
   },
   "source": [
    "\n",
    "### Autocorrelation of our random walk [analytic]\n",
    "\n",
    "Let's try this for the random walk for two points, 10 and 15:\n",
    "\n",
    "\\begin{align}\n",
    "E[X_{10}X_{15}] = E[(\\sum_{i=1}^{10}Y_i)(\\sum_{i=1}^{15}Y_i)].\n",
    "\\end{align}\n",
    "\n",
    "A bit of thought here tells us that if we expand out this product of sums and take their expectation values individually, $E[Y_iY_j]=\\delta_{ij}$, and so:\n",
    "\n",
    "\\begin{align}\n",
    "R(X_{10}X_{15}) = 10\\\\\n",
    "R(X_{t_1}X_{t_2})=\\min(t_1, t_2).\n",
    "\\end{align}\n",
    "\n",
    "So a random walk **isn't a wide-sense stationary process!!**\n",
    "\n",
    "### Wiener process\n",
    "\n",
    "Now, let's take this up a level to a continuous process. Let's define a white noise process $\\xi(t)$, where if you evaluate $\\xi(t=t_i)$ it is a value randomly drawn from a Gaussian distribution $\\cal N(0, 1)$. These are analogous to our \"steps.\" The autocorrelation function is:\n",
    "\n",
    "$$R(\\tau) = \\langle \\xi(t)\\xi(t+\\tau)\\rangle = \\delta(\\tau),$$\n",
    "\n",
    "where $\\delta(\\tau)$ is the dirac delta function.\n",
    "\n",
    "We then define our Wiener process in a similar way to our random walk, upgrading our sum to an integral:\n",
    "\n",
    "$$W_t = \\int_0^{t} \\xi(t')dt'.$$\n",
    "\n",
    "As with the random walk, now our auto-correlation function is:\n",
    "\n",
    "$$\\langle W_sW_t\\rangle = \\min(s, t).$$\n",
    "\n",
    "Additionally, just like a random walk (imagine setting $s=0$)\n",
    "\n",
    "$$(W_t - W_s) \\sim \\cal N(0, |t-s|).$$\n",
    "\n",
    "We can actually use this last property to simulate a Wiener process that is evaluated at discrete times. \n",
    "\n",
    "\\begin{align}\n",
    "W_0 &= 0\\\\\n",
    "W_{t_1} &= \\mathcal N(0, t_1)\\\\\n",
    "W_{t_2} &= \\mathcal N(t_1, |t_2 - t_1|)\\\\\n",
    "W_{t_3} &= \\mathcal N(t_2, |t_3 - t_2|)\\\\\n",
    "\\vdots\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8QSMZLcLcHX"
   },
   "outputs": [],
   "source": [
    "## Simulate a Wiener process\n",
    "# feel free to change these\n",
    "N_measurement_times = 2000 # number of measurements\n",
    "dt = 2 # time between_measurements\n",
    "\n",
    "# list of measurement times\n",
    "times = np.linspace(0, (N_measurement_times-1) * dt, num=N_measurement_times)\n",
    "\n",
    "# our Wiener process\n",
    "W_of_t = []\n",
    "\n",
    "# remember scale = standard deviation, and above in the text\n",
    "# we've given the variance\n",
    "# after some time, N(mean, variance)\n",
    "normDist = scipy.stats.norm(loc=0, scale=______)\n",
    "for ii in range(N_measurement_times):\n",
    "    if ii == 0:\n",
    "        W_of_t.append(0)\n",
    "    else:\n",
    "        # use scipy.stats.n\n",
    "        W_of_t.append(W_of_t[ii-1] + normDist.rvs())\n",
    "\n",
    "plt.plot(W_of_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwNoB06skyek"
   },
   "source": [
    "When we typically calculate and plot an autocorrelation function in practice, it tells us the correlation between two points separated by some lag $\\tau$. But for a Wiener process that doesn't really make much sense, because the correlation here doesn't just depend on the lag.\n",
    "\n",
    "However, we *can* try to characterize this process in a different way--using the [power-spectral density](https://en.wikipedia.org/wiki/Spectral_density). This describes the power in each frequency, $S(f)$. If we know it analytically, integrating this function over our frequency range should give us the variance, $\\int_0^{\\infty}S(f) df = \\sigma^2$.\n",
    "\n",
    "The PSD calculated from a set of discretely sampled data is defined as:\n",
    "\n",
    "$$S(f) = \\frac{1}{T_{obs}} |\\tilde x(f)|^2,$$\n",
    "\n",
    "where $T_{obs}$ is the time over which the Fourier transform is taken to get $\\tilde x(f)$.\n",
    "\n",
    "Let's calculate the PSD of our Wiener process. We use \"Welch's method,\" which means that we cut up the time-series data into (in our case 10) chunks, calculate the PSD in each chunk, and then average those 10 PSDs together. This means that we lose some low-frequency information, but get a better estimate of the PSD at all frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-0ankY_c_mv"
   },
   "outputs": [],
   "source": [
    "from scipy.signal import welch # method for calculating the PSD\n",
    "nperseg = int(times[-1] / (10*dt)) + 1 # 10 averages\n",
    "frequencies, Pxx = welch(W_of_t, fs=1/dt, nperseg=200)\n",
    "plt.loglog(frequencies, Pxx)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNW_ePZHoT5e"
   },
   "source": [
    "1.  In the cell below calculate the Variance from `Pxx` and `frequencies`. Compare it to `np.var(W_of_t)`. Do they agree? If not, why not? (*hint:* it has to do with Welch's method) \n",
    "\n",
    "2. Then go back up to where we simulated the Wiener process. Multiply `dt` by a factor of 10 to simulate a dataset that's even longer, and then re-run all of the cells including the one below. What has happened to the variances? The PSD?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p03Jj6wjoTdG"
   },
   "outputs": [],
   "source": [
    "deltaF = frequencies[1] - frequencies[0]\n",
    "# perform a simple Riemann sum over the PSD\n",
    "var_from_psd = np.sum(______)\n",
    "print('PSD integral:', var_from_psd)\n",
    "# calculate variance of the time series W_of_t\n",
    "print('Variance of time series:', _____)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6rDHvbuBufC"
   },
   "source": [
    "What happens if, instead of calculating the variance of the time-series itself, we high-pass filter it first, so we only include frequencies that are included in the PSD. Then does the variance agree with our PSD estimate?\n",
    "\n",
    "In the cell below, we also look at the difference between the low-pass filtered series and the original time-series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkp2EZDIBuBN"
   },
   "outputs": [],
   "source": [
    "from scipy.signal import butter, sosfilt\n",
    "# low pass filter so that we only include frequencies in the PSD, then calculate the variance\n",
    "# use a butterworth filter\n",
    "sos = butter(2, 1/(2 * nperseg * dt), btype='highpass', fs=1/dt, output='sos')\n",
    "print(sos.shape)\n",
    "print('Variance of low-pass filtered signal', np.var(sosfilt(sos, W_of_t)))\n",
    "print('Variance from PSD', np.sum(Pxx * deltaF))\n",
    "print('Variance from full time series', np.var(W_of_t))\n",
    "\n",
    "plt.plot(times, sosfilt(sos, W_of_t), label='low-pass filter')\n",
    "plt.plot(times, W_of_t, label='original')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Alt4NMtwpzS3"
   },
   "source": [
    "### What is the PSD of a Wiener process?\n",
    "\n",
    "There are a few ways to calculate it analytically. Both of which are a bit suspect. The [**Wiener-Khinchin Theoerem**](https://en.wikipedia.org/wiki/Wiener%E2%80%93Khinchin_theorem) says that the PSD (of a *wide sense stationary function*!!) is the Fourier transform of the autocorrelation function, $R(\\tau)$. We don't have a WSS function, but we can try it anyway and see what happens.\n",
    "\n",
    "If $$W_t = \\int_0^t dt' \\xi(t'),$$ then we can wave our hands and say that the Fourier transform of $W_t$, then should be $1/(2\\pi f)$ multiplied by the Fourier transform of $\\xi(t)$ (implicitly using integration by parts). Since we know the autocorrelation function for $\\xi(t)$, we can calculate its PSD first:\n",
    "\n",
    "$$S_\\xi(f) = \\int e^{2\\pi i f \\tau} \\delta(\\tau) d\\tau = 1.$$\n",
    "\n",
    "By our argument above, then\n",
    "\n",
    "$$S_W(f) = \\frac{1}{4\\pi^2 f^2}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5nT0y7zqLPn"
   },
   "outputs": [],
   "source": [
    "plt.loglog(frequencies, Pxx, label='Data')\n",
    "# factor of 2 to go to 1-sided PSD, which is what scipy.signal\n",
    "plt.loglog(frequencies, 2 * (4 * np.pi**2 * frequencies**2)**-1, label='Theory')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-EGkbvVvTvn"
   },
   "source": [
    "### Moral of this story?\n",
    "\n",
    "Stochastic processes are strange. Be careful when characterizing non-stationary processes, especially with \"standard\" PSDs used in signal processing. Your intuition will be weird! For example, if you keep taking data for a Wiener process, your PSD will get bigger. Those of us used to LIGO data will find that really (really) weird. \n",
    "\n",
    "But this is a common thing in PTAs. We often talk about $1/f^{\\alpha}$ noise, which is inherently non-stationary. So we have to be careful when characterizing it using a PSD or the Wiener-Khinchin Theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8bb83HNr18c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "statistics_intro.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
